---
title: "STAT350 Final Project - Team 16"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(latexpdf)
library(stringr)
library(ggplot2)
library(gridExtra)
library(faraway)
```
# Abstract
This is where the abstract will take place

# Introduction
This is where the introduction will take place

# Data Description

<br />

> About the Data - Red Wine Quality

First, we will read the red wine data and look at the fundamentals of the data. We will also generate a pairs plot and see the approximate relationship between the variables.

```{r, eval=TRUE}
red.wine = read.csv("winequality-red.csv", header=TRUE, sep=";")
names(red.wine)
dim(red.wine)
sapply(red.wine, class)
red.info = summary(red.wine)
```
<br />
Include some basic descriptions including the class and the descriptive statistics of the variables. 

<br />
```{r, fig.width = 7, fig.height = 7}
pairs(red.wine)
```

Include some basic description regarding the pairs plot.

<br />


<br />
Include some basic descriptions including the class and the descriptive statistics of the variables. 


> New Additional Data Point

Before we begin the data analysis, we will introduce the median point as a new additional data point to the white wine data.

```{r}
red.datapoints = vector(mode = 'numeric', length = 12)

for (i in 1:12) {
red.med = as.numeric(str_extract(red.info[3,i], "\\d+\\.*\\d*")) #extracts value from summary
red.datapoints[i] = red.med
}

red.wine = rbind(red.wine, red.datapoints)
```

We now introduced the median point as a new point to the red wine quality data.

<br />


> Data Visualization - Histograms

Now for the last step before going into fitting the model into a linear model, we will generate several histograms for the all the variables. This will help us see the approximate relationship between each varibles and wine quality.

```{r}
hist_y = ggplot(aes(quality), data = red.wine) +
  geom_histogram(aes(color=I('black'), fill=I('firebrick')), binwidth = 1) +
  ggtitle('Histogram of Quality') + theme(plot.title = element_text(size=9))

hist_x1 = ggplot(aes(fixed.acidity), data = red.wine) +
  geom_histogram(aes(color=I('black'), fill=I('firebrick')), binwidth = 0.2) +
  ggtitle('Histogram of Fixed Acidity') + theme(plot.title = element_text(size=8))

hist_x2 = ggplot(aes(volatile.acidity), data = red.wine) +
  geom_histogram(aes(color=I('black'), fill=I('firebrick')), binwidth = 0.02) +
  ggtitle('Histogram of Volatile Acidity') + theme(plot.title = element_text(size=7.5))

hist_x3 = ggplot(aes(citric.acid), data = red.wine) +
  geom_histogram(aes(color=I('black'), fill=I('firebrick')), binwidth = 0.05) +
  ggtitle('Histogram of Citric Acid') + theme(plot.title = element_text(size=8))

hist_x4 = ggplot(aes(residual.sugar), data = red.wine) +
  geom_histogram(aes(color=I('black'), fill=I('firebrick')), binwidth = 0.5) +
  ggtitle('Histogram of Residual Sugar') + theme(plot.title = element_text(size=6.7))

hist_x5 = ggplot(aes(chlorides), data = red.wine) +
  geom_histogram(aes(color=I('black'), fill=I('firebrick')), binwidth = 0.05) +
  ggtitle('Histogram of Chlorides') + theme(plot.title = element_text(size=8.5))

hist_x6 = ggplot(aes(free.sulfur.dioxide), data = red.wine) +
  geom_histogram(aes(color=I('black'), fill=I('firebrick')), binwidth = 3) +
  ggtitle('Histogram of Free Sulfur Dioxide') + theme(plot.title = element_text(size=6))

hist_x7 = ggplot(aes(total.sulfur.dioxide), data = red.wine) +
  geom_histogram(aes(color=I('black'), fill=I('firebrick')), binwidth = 8) +
  ggtitle('Histogram of Total Sulfur Dioxide') + theme(plot.title = element_text(size=6))

hist_x8 = ggplot(aes(density), data = red.wine) +
  geom_histogram(aes(color=I('black'), fill=I('firebrick')), binwidth = 0.0005) +
  ggtitle('Histogram of Density') + theme(plot.title = element_text(size=9))

hist_x9 = ggplot(aes(pH), data = red.wine) +
  geom_histogram(aes(color=I('black'), fill=I('firebrick')), binwidth = 0.02) +
  ggtitle('Histogram of pH') + theme(plot.title = element_text(size=9))

hist_x10 = ggplot(aes(sulphates), data = red.wine) +
  geom_histogram(aes(color=I('black'), fill=I('firebrick')), binwidth = 0.1) +
  ggtitle('Histogram of Sulphates') + theme(plot.title = element_text(size=8))

hist_x11 = ggplot(aes(alcohol), data = red.wine) +
  geom_histogram(aes(color=I('black'), fill=I('firebrick')), binwidth = 0.3) +
  ggtitle('Histogram of Alcohol') + theme(plot.title = element_text(size=9))

grid.arrange(hist_y, hist_x1, hist_x2, hist_x3, hist_x4, hist_x5, hist_x6, hist_x7, hist_x8, hist_x9, hist_x10, hist_x11, ncol = 4)
```
<br />

There are a couple of things that we can see from the histograms above.

* Quality variable seem to have a normal distribution, with most of the values concentrated in categories 5, 6, and 7. In other words, there are much more normal wines than excellent or poor ones.
* Most of the variables have a very heavy right-tail, except for quality, pH, sulphates, and alcohol.
* Some of the skewed data such as fixed acidity, volatile acidity, or citric acid can be normal when the outliers are detected and fixed.

<br />


> Data Visualization - Regressors vs. Predictor

Now for the last step before going into fitting the model into a linear model, we will plot each of the regressors against the predictor. This will help us see the approximate relationship between each varibles and wine quality.

```{r}
group_x1 = ggplot(aes(fixed.acidity, quality), data = red.wine) + 
  ggtitle("Fixed Acidity vs Quality") +
  geom_jitter(width = 0.25, alpha = 0.1, colour = "firebrick3") + 
  geom_smooth(method = "lm", se = FALSE, colour = "black") + theme(plot.title = element_text(size=9))

group_x2 = ggplot(aes(volatile.acidity, quality), data = red.wine) + 
  ggtitle("Volatile Acidity vs Quality") +
  geom_jitter(width = 0.25, alpha = 0.1, colour = "firebrick3") + 
  geom_smooth(method = "lm", se = FALSE, colour = "black") + theme(plot.title = element_text(size=8.5))

group_x3 = ggplot(aes(citric.acid, quality), data = red.wine) + 
  ggtitle("Citric Acid vs Quality") +
  geom_jitter(width = 0.25, alpha = 0.1, colour = "firebrick3") + 
  geom_smooth(method = "lm", se = FALSE, colour = "black") + theme(plot.title = element_text(size=9))

group_x4 = ggplot(aes(residual.sugar, quality), data = red.wine) + 
  ggtitle("Residual Sugar vs Quality") +
  geom_jitter(width = 0.25, alpha = 0.1, colour = "firebrick3") + 
  geom_smooth(method = "lm", se = FALSE, colour = "black") + theme(plot.title = element_text(size=8))

group_x5 = ggplot(aes(chlorides, quality), data = red.wine) + 
  ggtitle("Chlorides vs Quality") +
  geom_jitter(width = 0.25, alpha = 0.1, colour = "firebrick3") + 
  geom_smooth(method = "lm", se = FALSE, colour = "black") + theme(plot.title = element_text(size=9))

group_x6 = ggplot(aes(free.sulfur.dioxide, quality), data = red.wine) + 
  ggtitle("Free Sulfur Dioxide vs Quality") +
  geom_jitter(width = 0.25, alpha = 0.1, colour = "firebrick3") + 
  geom_smooth(method = "lm", se = FALSE, colour = "black") + theme(plot.title = element_text(size=7))

group_x7 = ggplot(aes(total.sulfur.dioxide, quality), data = red.wine) + 
  ggtitle("Total Sulfur Dioxide vs Quality") +
  geom_jitter(width = 0.25, alpha = 0.1, colour = "firebrick3") + 
  geom_smooth(method = "lm", se = FALSE, colour = "black") + theme(plot.title = element_text(size=6.5))

group_x8 = ggplot(aes(density, quality), data = red.wine) + 
  ggtitle("Density vs Quality") +
  geom_jitter(width = 0.25, alpha = 0.1, colour = "firebrick3") + 
  geom_smooth(method = "lm", se = FALSE, colour = "black") + theme(plot.title = element_text(size=9))

group_x9 = ggplot(aes(pH, quality), data = red.wine) + 
  ggtitle("pH vs Quality") +
  geom_jitter(width = 0.25, alpha = 0.1, colour = "firebrick3") +  
  geom_smooth(method = "lm", se = FALSE, colour = "black") + theme(plot.title = element_text(size=9))

group_x10 = ggplot(aes(sulphates, quality), data = red.wine) + 
  ggtitle("Sulphates vs Quality") +
  geom_jitter(width = 0.25, alpha = 0.1, colour = "firebrick3") + 
  geom_smooth(method = "lm", se = FALSE, colour = "black") + theme(plot.title = element_text(size=9))

group_x11 = ggplot(aes(alcohol, quality), data = red.wine) + 
  ggtitle("Alcohol vs Quality") +
  geom_jitter(width = 0.25, alpha = 0.1, colour = "firebrick3") + 
  geom_smooth(method = "lm", se = FALSE, colour = "black") + theme(plot.title = element_text(size=9))
```

```{r, message = FALSE}
grid.arrange(group_x1, group_x2, group_x3, group_x4, group_x5, group_x6, group_x7, group_x8, group_x9, group_x10, group_x11, ncol = 4)
```
<br />

There are a couple of things that we can see from the plots above.

* Fixed acidity, citric acid, sulphates, and alcohol have a positive relationship with quality.
* Volatile acidity, total sulfur dioxide, and pH have a negative relationship with quality.
* Rest of the data shows a fairly random pattern.

<br />


```{r}
par(mfrow = c(1,4))
boxplot(red.wine$quality, col="slategray2", pch=19, xlab = "quality")
boxplot(red.wine$fixed.acidity, col="slategray2", pch=19, xlab = "Fixed Acidity")
boxplot(red.wine$volatile.acidity, col="slategray2", pch=19, xlab = "Volatile Acidity")
boxplot(red.wine$citric.acid, col="slategray2", pch=19, xlab = "Citric Acid")
boxplot(red.wine$residual.sugar, col="slategray2", pch=19, xlab = "Residual Sugar")
boxplot(red.wine$chlorides, col="slategray2", pch=19, xlab = "Chlorides")
boxplot(red.wine$free.sulfur.dioxide, col="slategray2", pch=19, xlab = "Free Sulfur Dioxide")
boxplot(red.wine$total.sulfur.dioxide, col="slategray2", pch=19, xlab = "Total Sulfur Dioxide")
boxplot(red.wine$density, col="slategray2", pch=19, xlab = "Density")
boxplot(red.wine$pH, col="slategray2", pch=19, xlab = "pH")
boxplot(red.wine$sulphates, col="slategray2", pch=19, xlab = "Sulphates")
boxplot(red.wine$alcohol, col="slategray2", pch=19, xlab = "Alcohol")
```
<br />

There are a couple of things that we can see from the boxplots above.

* Citric acid, free sulfur dioxide, total sulfur dioxide, and alcohol are right-skewed. The skewness of the data would not be fixed even after eliminating the outliers.
* For other variables, detecting the outliers and fixing them will make the distribution more symmetric. However, the some of the outliers seem very far away from the data.
* Except for density, most of the outliers are on the larger side of the data.

<br />


# Methods

<br />

## Multiple Linear Regression - Full Model
<br />

> Pairs Plot

First, with red wine, conduct a pairs plot to see if there are any particular patterns.

```{r}
pairs(red.wine)
```

The pairs plot tells us a bit about the relationship between variables in the dataset. Specifically, we can see that a linear model seems appropriate, although some of the variables seem to have less of a linear relationship (we can look into that by conducting hypothesis tests). Also, we have to keep in mind of the multicollinearity present among the explanatory variables. Now, we will conduct a linear regression against the quality of the red wine.
<br />

> Fitting Into a Linear Model

```{r}
red.mdl = lm(quality~., data=red.wine)
summary(red.mdl)
```

There are a couple of things that we notice from the summary of the linear model. 

* Fixed acidity, residual sugar, free sulfur dioxide, sulphates, and alcohol have a positive relationship with red wine quality.
* Volatile acidity, citric acid, chlorides, total sulfur dioxide, density, and pH have a negative relationship with red wine quality.
* The P-value of the overall regression is significantly small, which suggests that there is a linear association between at least one of the regressors and the red wine quality.
* The adjusted R-squared value is 0.3561 (35.61%), which implies that our linear model fit to the data is not satisfactory. In fact, only 35% of the data can be explained by the model.
* Looking at the individual P-values, fixed acidity, citric acid, residual sugar and density seem to be insignificant given that all the other variables are in the model. 

In order to see if some of the regressors are insignificant to the regression, we will first run the anova test.

> Anova Test

```{r}
anova(red.mdl)
```

The anova test conducts a partial F-test, and supports our hypothesis that citric acid and residual sugar variable seem to be insiginificant to the linear model given that the variables before them are already included in the model.

To check which of the variables must be selected in building the model, we will conduct a variable selection later on.

> Hidden Extrapolation

It is fairly easy to inadvertently extrapolate when doing multiple regression, so we will go through the formal procedure to look for hidden extrapolation. First, we will obtain the hat matrix to see if certain data points are within the regressor variable hull (RVH).

```{r}
X = cbind(c(1, nrow(red.wine)), red.wine$fixed.acidity, red.wine$volatile.acidity, red.wine$citric.acid, red.wine$residual.sugar, red.wine$chlorides, red.wine$free.sulfur.dioxide, red.wine$total.sulfur.dioxide, red.wine$density, red.wine$pH, red.wine$sulphates, red.wine$alcohol, red.wine$quality)
H = X %*% solve(t(X) %*% X) %*% t(X)
hii = diag(H)
Hmax = max(diag(H))
plot(hii)
```
<br />

Using the identify() function, we can see that the 152, 259, and 481st indeces from the $h_{ii}$ matrix look a bit far away from other points. Hence, we will check if we are unknowingly extrapolating for these points.

```{r}
point1 = hii[152]
point2 = hii[259]
point3 = hii[481]
print(c(point1, point2, point3, Hmax))
```
<br />

Comparing the values to the $h_{max}$, all the values are inside the RVH. Hence, we conclude all data points fall under interpolation and is safe to make predictions at given our current model.

> Multicollinearity

In the pairs plot above, we suspected that there may be near linear relationship between some of the regressor variables in the data. We will conduct a diagnostics since multicollinearity will cause a serious problem that may dramatically impact the usefulness of the linear model. We will use the variance inflation factor (VIF) to check.

```{r}
vif(red.mdl)
```
<br />

Although VIF for fixed acidity are large, but it is not larger than 10, the rule-of-thumb that our textbook suggests. In fact, none of the VIFs are larger than 10, so we will conclude that there are no serious issues regarding multicollinearity. 

# Model Assessment
<br />

> Residual Analysis

We will first go over the graphic analysis of the residuals to check if the error are i.i.d. normally distributed, with zero mean and constant variance.

##### Residual vs. Fitted

```{r}
plot(red.mdl, which=1)
```
<br />

Comments on the residual vs. fitted plot

<br />


##### Normal Q-Q

```{r}
plot(red.mdl, which=2)
```
<br />

We observe that the residuals for our model meet the normality assumption. There seems to be a slight negative skew to the distribution of the residuals, although it is not too much of a concern. Also, there are some potential influential points. We will look at this in a moment. 

<br />


##### Scale-Location

```{r}
plot(red.mdl, which=3)
```
<br />

Comments on the Scale-Location Plot

<br />


##### Residuals vs. Leverage

```{r}
plot(red.mdl, which=5)
```
<br />

Comments on the Residuals vs. Leverage Plot

<br />

> Outlier Detection

Outliers are data points that are not typical of the rest of the data. They can either improve or worsen the fit of the equation, so it is important to investigate each outliers or potential influential points.

=======
For the outlier detection, we are going to use externally studentized residuals and plot them against various values.

```{r}
ti = rstudent(red.mdl)
plot(red.mdl$fitted.values, ti)
```
<br / >

Comments on the residual vs. fitted values plot. 

Identifying the points on the plot, 653 and 833rd data are potential outliers. To investigate the influence of these points on the model, we will obtain an equation with these two observations deleted.

```{r}
red.wine.del = red.wine[-c(653,833), ]
red.mdl.del = lm(quality ~ . , data=red.wine.del)
red.sum = summary(red.mdl)
red.sum.del = summary(red.mdl.del)
red.mse = mean(red.sum$residuals^2)
red.mse.del = mean(red.sum.del$residuals^2)
red.sum
red.sum.del
red.mse
red.mse.del
```
<br />

Deleting the potential outlier points has almost no effect on the estimates of the regression coefficients nor on the residual mean square. In fact, deleting the points causes a slight increase in $R^2$, even though the increase does not seem significant. Hence, we conclude that points 653 and 833 are not influential.

Based on the information we acquired from both the residuals vs. fitted values, we may say that the deficiency of the model is due to trying to fit a discrete quality value to a continuous data. 

<br />

> Outlier Diagnostics : Cook's Distance

Cook's distance is one of the ways to measure a point's influence by considering both the location of a point in the x space and the response variable. Points with large values of $D_i$ can be interpreted as an influential point.

```{r}
red.cooksd = cooks.distance(red.mdl)
max.di = max(red.cooksd)
plot(red.cooksd, pch="*", cex=2, main="Influential Observations by Cook's Distance") 
abline(h = 10*mean(red.cooksd, na.rm=T), col="red")  # adding the cutoff line
text(x=1:length(red.cooksd)+1, y=red.cooksd, labels=ifelse(red.cooksd>10*mean(red.cooksd, na.rm=T),names(red.cooksd),""), col="red")
```
<br />

When interpreting Cook's distance, we categorize the values as follows:

* $D_i$ > 0.5 : The $i_{th}$ data point is worthy of further investigation as influential
* $D_i$ > 1 : The $i_{th}$ data point is quite likely to be influential
* If $D_i$ sticks out from the other values, it is most likely to be influential

Even though the maximum value for the Cook's distance is 0.06885, which is even smaller than 0.5, but since it is still larger than other $D_i$ values, we will take a closer look.

```{r}
red.wine.del2 = red.wine[-152, ]
red.mdl.del2 = lm(quality ~ . , data=red.wine.del2)
red.sum.del2 = summary(red.mdl.del2)
red.mse.del2 = mean(red.sum.del2$residuals^2)
red.sum$coefficients
red.sum.del2$coefficients
red.mse
red.mse.del2
```
<br />

Unfortunately, deleting the point causes a decrease in $R^2$ and an increase in mean squared errors, which is not seem desirable. Hence, we conclude that the point 152 is not influential.

As mentioned above, this may be due to trying to fit a discrete quality value to a continuous data. 

<br />

# Transformation
<br />

Previously, the summary of the full model has told us that our model is not doing a good job in fitting the data. And while analyzing the residuals, we realized the fact that fitting a discrete data into a continuous model may be causing such problem. 

<br />

> Log Transformation

First, let's try a log transformation to see if it will improve our regression.

```{r}
red.log = lm(log(quality) ~ ., data=red.wine)
red.log.sum = summary(red.log)
red.sum
red.log.sum
```
<br />

By taking logarthims of quality values, estimates of each of the coefficients changed quite a lot. However, the log transformation decreased the $R^2$ value, which is not good.

Since our goal was to improve the model to better fit the data, we will skip further analysis on the log transformation model and move on.

<br />

> Square-Root Transformation

Next, we will do a square-root transformation to see if it will improve our regression.

```{r}
red.sqr = lm(sqrt(quality) ~ ., data=red.wine)
red.sqr.sum = summary(red.sqr)
red.sum
red.sqr.sum
```
<br />

By taking the square-root of quality values, estimates of each of the coefficients changed quite a lot. However, the square-root transformation decreased the $R^2$ value, which is not good.

Since our goal was to improve the model to better fit the data, we will skip further analysis on the square-root transformation model and move on.

<br />

> Transformation On Ordinal Variables

Taking the discreteness of the quality values into consideration, we will now look into transforming ordinal variables. 
>>>>>>> 66422d98b2cf08c8b8037169e05fdc9ee62efa2c 




> Cross Validation 

In this section we perform cross validation in order to see how well our model predicts the quality of red wine.We do this by comparing the actual value to the predicted value. We do this by dividing the dataset in such a way that 80 percent of the dataset is part of the training set and 20 percent of the dataset is the testing set. We've repeated the steps of validation 5 times to check the values of R squared, root mean square error and mean absolute error which would show us how the model behaves.
```{r}
setwd("C:/Users/surbh/Desktop")
getwd()
library(Metrics)
r_w<- read.csv("winequality-red.csv", header=TRUE,sep=";")
attach(r_w)
detach(r_w)
set.seed(71168)
no_of_samples = ceiling(0.8*length(r_w$quality))

for(i in 1:5){

training_sample = sample(c(1:length(r_w$quality)),no_of_samples)
training_sample = sort(training_sample)

train_data <- r_w[training_sample,]
test_data<- r_w[-training_sample,]

trained_model<- lm(train_data$quality~., data = train_data)
summary(trained_model) 

preds1 <- predict(trained_model,test_data)

plot(test_data$quality,preds1,xlim=c(4,10),ylim=c(4,10))
abline(c(0,1))

R.sq=summary(lm(quality~., data = train_data))$r.squared
root_mean_square_error = rmse(preds1, test_data$quality)
mean_abs_error = mae(preds1, test_data$quality)
print(c(i,R.sq,root_mean_square_error,mean_abs_error))
}
```



From our numerical observation, the first thing that we see is the R squared value, it remains to be small and it shows that only 35 percent of the model could be explained whereas an ideal model should have atleast 80 . The next observation is the root mean square error which is the difference between the predicted quality and test quality, for a good model we expect to have smaller differences but in our case it goes up to 0.65. Lastly we look at the mean square errors. Again, the values should be small but in our case it is relatively large which further implies that our model isn't the best.

From our graphical observations, we see that when it comes to predicting the quality 4, the model tends to overestimate the value and when it comes to predicting quality 8, the model underestimates the value. Our model does somewhat predict quality 5 and 6 but there is a huge amount of spread in those areas, that shows that many points are further away from the centre which isn't a good sign


>>>>>>>>>>>>>>>>>>>>>>>>



